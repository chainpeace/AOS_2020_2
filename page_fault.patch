diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index c29976eca4a8..6a4d50bcf8fc 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -358,6 +358,9 @@
 434	common	pidfd_open		__x64_sys_pidfd_open
 435	common	clone3			__x64_sys_clone3/ptregs
 
+#ihhwang
+436 common plmt_set_comm    __x64_sys_plmt_set_comm
+
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
 # for native 64-bit operation. The __x32_compat_sys stubs are created
diff --git a/fs/exec.c b/fs/exec.c
index d62cd1d71098..ec8b7d74d765 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -78,6 +78,9 @@ int suid_dumpable = 0;
 static LIST_HEAD(formats);
 static DEFINE_RWLOCK(binfmt_lock);
 
+//ihhwang
+extern int is_plmt_process(const char *process_name);
+
 void __register_binfmt(struct linux_binfmt * fmt, int insert)
 {
 	BUG_ON(!fmt);
@@ -1377,6 +1380,13 @@ void setup_new_exec(struct linux_binprm * bprm)
 	perf_event_exec();
 	__set_task_comm(current, kbasename(bprm->filename), true);
 
+	/* ihhwang */
+	if(is_plmt_process(current->comm)){
+		printk("this is plmt_process : %s\n", current->comm);
+		current->mm->plmt_enable = 1;
+	}
+	//
+
 	/* Set the new mm task size. We have to do that late because it may
 	 * depend on TIF_32BIT which is only updated in flush_thread() on
 	 * some architectures like powerpc
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 270aa8fd2800..a85eff03d85b 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -526,11 +526,21 @@ struct mm_struct {
 		struct work_struct async_put_work;
 	} __randomize_layout;
 
+	//ihhwang
+	//unsigned long this_page_fault_address;
+	unsigned long prev_page_fault_address;
+	pte_t *prev_page_fault_pte;
+	unsigned int plmt_enable;
+	unsigned long page_fault_cnt; 
+	//
+	
 	/*
 	 * The mm_cpumask needs to be at the end of mm_struct, because it
 	 * is dynamically sized based on nr_cpu_ids.
 	 */
 	unsigned long cpu_bitmap[];
+
+
 };
 
 extern struct mm_struct init_mm;
diff --git a/include/linux/plmt.h b/include/linux/plmt.h
new file mode 100644
index 000000000000..d63ad7b40930
--- /dev/null
+++ b/include/linux/plmt.h
@@ -0,0 +1,22 @@
+#ifndef _LINUX_PLMT_H
+#define _LINUX_PLMT_H
+
+#define MAX_NAME_LEN 32
+//#define PTE_FAKE_BIT 51
+
+/// 9th bit  _PAGE_BIT_SOFTW1
+#define PTE_FAKE_MASK _PAGE_SOFTW1
+//#define PTE_FAKE_MASK (_AT(pteval_t, 1) << PTE_FAKE_BIT)
+
+char plmt_process_name[MAX_NAME_LEN];
+
+inline pte_t pte_mkfake(pte_t pte);
+inline pte_t pte_clrfake(pte_t pte);
+inline int is_pte_fake(pte_t pte);
+
+inline pte_t pte_clrpresent(pte_t pte);
+
+long plmt_set_comm(const char __user *process_name);
+int is_plmt_process(const char * process_name);
+
+#endif
\ No newline at end of file
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index f7c561c4dcdd..352355bd6f47 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -1215,6 +1215,8 @@ asmlinkage long sys_mmap_pgoff(unsigned long addr, unsigned long len,
 			unsigned long fd, unsigned long pgoff);
 asmlinkage long sys_old_mmap(struct mmap_arg_struct __user *arg);
 
+// ihhwang
+asmlinkage long sys_plmt_set_comm(const char __user * process_name);
 
 /*
  * Not a real system call, but a placeholder for syscalls which are
diff --git a/kernel/Makefile b/kernel/Makefile
index daad787fb795..ee7ebc9fc41b 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -10,7 +10,8 @@ obj-y     = fork.o exec_domain.o panic.o \
 	    extable.o params.o \
 	    kthread.o sys_ni.o nsproxy.o \
 	    notifier.o ksysfs.o cred.o reboot.o \
-	    async.o range.o smpboot.o ucount.o
+	    async.o range.o smpboot.o ucount.o \
+	    plmt.o
 
 obj-$(CONFIG_MODULES) += kmod.o
 obj-$(CONFIG_MULTIUSER) += groups.o
diff --git a/kernel/plmt.c b/kernel/plmt.c
new file mode 100644
index 000000000000..aa4eea8f61f4
--- /dev/null
+++ b/kernel/plmt.c
@@ -0,0 +1,49 @@
+#include <linux/syscalls.h>
+#include <linux/plmt.h>
+#include <asm/pgtable.h>
+
+//reseve bit 51st
+
+
+//native_pte_val(pte)
+//pte_flags(pte)
+inline pte_t pte_mkfake(pte_t pte){
+
+    return pte_set_flags(pte, PTE_FAKE_MASK);
+}
+inline pte_t pte_clrfake(pte_t pte){
+    return pte_clear_flags(pte, PTE_FAKE_MASK);
+}
+inline int is_pte_fake(pte_t pte){
+    if(native_pte_val(pte) & PTE_FAKE_MASK)
+        return 1;
+    else 
+        return 0;
+}
+//inline pte_t pte_mkpresent(pte_t pte){}
+inline pte_t pte_clrpresent(pte_t pte){
+
+    return pte_clear_flags(pte, _PAGE_PRESENT);
+}
+
+
+
+//system call to enable plmt
+SYSCALL_DEFINE1(plmt_set_comm, const char __user *, process_name){
+
+    long ret;
+    printk("set plmt process name : %s\n", process_name);
+    ret = strncpy_from_user(plmt_process_name, process_name, MAX_NAME_LEN-1);
+
+    return ret;
+}
+
+// check this is plmt process 
+int is_plmt_process(const char * process_name){
+    
+    if(!strncmp(process_name, plmt_process_name, MAX_NAME_LEN))
+        return 1;
+
+    return 0;
+    
+}
\ No newline at end of file
diff --git a/mm/memory.c b/mm/memory.c
index cb7c940cf800..d829aa381e85 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -82,6 +82,14 @@
 
 #include "internal.h"
 
+//ihhwang
+extern inline pte_t pte_mkfake(pte_t pte);
+extern inline pte_t pte_clrfake(pte_t pte);
+extern inline int is_pte_fake(pte_t pte);
+extern inline pte_t pte_clrpresent(pte_t pte);
+
+
+
 #if defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS) && !defined(CONFIG_COMPILE_TEST)
 #warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.
 #endif
@@ -3615,7 +3623,7 @@ static vm_fault_t do_fault(struct vm_fault *vmf)
 			vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm,
 						       vmf->pmd,
 						       vmf->address,
-						       &vmf->ptl);
+						       &vmf->ptl); 
 			/*
 			 * Make sure this is not a temporary clearing of pte
 			 * by holding ptl and checking again. A R/M/W update
@@ -3821,6 +3829,38 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
 static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 {
 	pte_t entry;
+	
+	
+	// ihhwang
+	//pte_t = pte_clear_flag(pte_t pte, _PAGE_PRESENT)
+	//_PAGE_USER --> already in user space
+	//static inline pte_t pte_mknotpresent(pte_t pte)
+	vm_fault_t ret;
+	unsigned int enable_plmt = 0;
+	pte_t *prev_pte;
+
+	if(current && current->mm && current->mm->plmt_enable){
+		char mode;
+		char type;
+		enable_plmt = 1;
+		prev_pte = current->mm->prev_page_fault_pte;
+
+		if(vmf->flags & FAULT_FLAG_WRITE)
+			mode = 'w';
+		else
+			mode = 'r';
+
+		if(vmf->flags & FAULT_FLAG_INSTRUCTION)
+			type = 'i';
+		else 
+			type = 'd';
+
+		printk("%lu : this page address : %lx, mode : %c, type : %c ----",++(vmf->vma->vm_mm->page_fault_cnt), 
+			vmf->address & PAGE_MASK, mode, type);
+		printk("prev page address : %lx", vmf->vma->vm_mm->prev_page_fault_address & PAGE_MASK);
+		vmf->vma->vm_mm->prev_page_fault_address = vmf->address;	
+	}
+	//
 
 	if (unlikely(pmd_none(*vmf->pmd))) {
 		/*
@@ -3857,17 +3897,81 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 			vmf->pte = NULL;
 		}
 	}
+	
+	if(enable_plmt){
+		printk("origianl pte : %lx", pte_val(vmf->orig_pte));
+		if(vmf->pte){
+			printk("vmf->pte exist : %lx", pte_val(*vmf->pte));
+			
+			if(is_pte_fake(*vmf->pte)){
+			
+				//*prev_pte = pte_set_flags(*prev_pte, _PAGE_PRESENT); // set present bit
+				//*prev_pte = pte_mkpresent(*prev_pte);
+				*vmf->pte = pte_clrfake(*vmf->pte);
+				
+				//current->mm->prev_page_fault_pte = prev_pte;
+
+				printk("^-- this is FAKE fault--^");
+				return VM_FAULT_MAJOR;
+			}
+		}else {
+			printk("vmf->pte not exist");
+		}
+		
+	}
 
 	if (!vmf->pte) {
-		if (vma_is_anonymous(vmf->vma))
-			return do_anonymous_page(vmf);
-		else
-			return do_fault(vmf);
+		if (vma_is_anonymous(vmf->vma)){
+			
+
+			ret = do_anonymous_page(vmf);
+			if(enable_plmt){//ihhwang
+				printk("^-- this is anonymous page fault"); 
+				//static inline void pte_free(struct mm_struct *mm, struct page *pte_page) //pgalloc.h
+				
+				if(prev_pte){
+					//*prev_pte = pte_clrpresent(*prev_pte);
+					*prev_pte = pte_mkfake(*prev_pte);
+				}
+				current->mm->prev_page_fault_pte = vmf->pte;
+				//*vmf->pte = pte_mkfake(*vmf->pte);
+				//printk("pte value : %lx", pte_val(*vmf->pte))
+			}
+			return ret;
+		}
+		else{	
+			ret = do_fault(vmf);
+			if(enable_plmt){
+				printk("^-- this is file mapped page fault"); //ihhwang
+				//static inline void pte_free(struct mm_struct *mm, struct page *pte_page) //pgalloc.h
+				
+				if(prev_pte){
+					//*prev_pte = pte_clrpresent(*prev_pte);
+					*prev_pte = pte_mkfake(*prev_pte);
+				}
+				current->mm->prev_page_fault_pte = vmf->pte;
+				//*vmf->pte = pte_mkfake(*vmf->pte);
+				//printk("pte value : %lx", pte_val(*vmf->pte))
+			}
+			return ret;
+		}
 	}
 
-	if (!pte_present(vmf->orig_pte))
-		return do_swap_page(vmf);
+	if (!pte_present(vmf->orig_pte)){
+		ret = do_swap_page(vmf);
+		if(enable_plmt){
+			printk("^-- this is swapped page fault"); //ihhwang
 
+			if(prev_pte){
+			//*prev_pte = pte_clrpresent(*prev_pte);
+			*prev_pte = pte_mkfake(*prev_pte);
+			}
+			current->mm->prev_page_fault_pte = vmf->pte;
+			//*vmf->pte = pte_mkfake(*vmf->pte);
+			//printk("pte value : %lx", pte_val(*vmf->pte))
+		}
+		return ret;
+	}
 	if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
 		return do_numa_page(vmf);
 
@@ -3877,8 +3981,20 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	if (unlikely(!pte_same(*vmf->pte, entry)))
 		goto unlock;
 	if (vmf->flags & FAULT_FLAG_WRITE) {
-		if (!pte_write(entry))
-			return do_wp_page(vmf);
+		if (!pte_write(entry)){
+			ret = do_wp_page(vmf);
+			if(enable_plmt){
+				printk("^-- this is write protected page fault"); //ihhwang		
+				if(prev_pte){
+				//*prev_pte = pte_clrpresent(*prev_pte);
+				*prev_pte = pte_mkfake(*prev_pte);
+				}
+				current->mm->prev_page_fault_pte = vmf->pte;
+				//*vmf->pte = pte_mkfake(*vmf->pte);
+				//printk("pte value : %lx", pte_val(*vmf->pte))
+			}
+			return ret;
+		}
 		entry = pte_mkdirty(entry);
 	}
 	entry = pte_mkyoung(entry);
@@ -3998,33 +4114,47 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		unsigned int flags)
 {
+	/*
+	if(current->mm->plmt_enable){
+		printk("comm : %s\n", current->comm);
+		printk("handle mm fault start. address : %lu\n", address);
+	}
+	*/
 	vm_fault_t ret;
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("handle_mm_fault start %lx", address);
 	__set_current_state(TASK_RUNNING);
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("set current state done");
 	count_vm_event(PGFAULT);
 	count_memcg_event_mm(vma->vm_mm, PGFAULT);
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("count event done");
 
 	/* do counter updates before entering really critical section. */
 	check_sync_rss_stat(current);
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("check rss stat done done");
 	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
 					    flags & FAULT_FLAG_INSTRUCTION,
 					    flags & FAULT_FLAG_REMOTE))
 		return VM_FAULT_SIGSEGV;
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("arch vma access permitted done");
 	/*
 	 * Enable the memcg OOM handling for faults triggered in user
 	 * space.  Kernel faults are handled more gracefully.
 	 */
 	if (flags & FAULT_FLAG_USER)
 		mem_cgroup_enter_user_fault();
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("mem cgroup enter user fault done");
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
 		ret = __handle_mm_fault(vma, address, flags);
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("handle fault done");
 	if (flags & FAULT_FLAG_USER) {
 		mem_cgroup_exit_user_fault();
 		/*
@@ -4033,8 +4163,10 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		 * VM_FAULT_OOM), there is no need to kill anything.
 		 * Just clean up the OOM state peacefully.
 		 */
-		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
-			mem_cgroup_oom_synchronize(false);
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("mem_cgroup exit done");
+	if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
+		mem_cgroup_oom_synchronize(false);
 	}
 
 	return ret;
