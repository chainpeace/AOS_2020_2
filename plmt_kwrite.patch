diff --git a/Makefile b/Makefile
index 29948bc4a0d2..cc72b8472f24 100644
--- a/Makefile
+++ b/Makefile
@@ -1,7 +1,7 @@
 # SPDX-License-Identifier: GPL-2.0
 VERSION = 5
 PATCHLEVEL = 4
-SUBLEVEL = 58
+SUBLEVEL = 59
 EXTRAVERSION =
 NAME = Kleptomaniac Octopus
 
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index c29976eca4a8..6a4d50bcf8fc 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -358,6 +358,9 @@
 434	common	pidfd_open		__x64_sys_pidfd_open
 435	common	clone3			__x64_sys_clone3/ptregs
 
+#ihhwang
+436 common plmt_set_comm    __x64_sys_plmt_set_comm
+
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
 # for native 64-bit operation. The __x32_compat_sys stubs are created
diff --git a/fs/exec.c b/fs/exec.c
index d62cd1d71098..a3c48b4ffdeb 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -78,6 +78,9 @@ int suid_dumpable = 0;
 static LIST_HEAD(formats);
 static DEFINE_RWLOCK(binfmt_lock);
 
+//ihhwang
+extern int is_plmt_process(const char *process_name);
+
 void __register_binfmt(struct linux_binfmt * fmt, int insert)
 {
 	BUG_ON(!fmt);
@@ -1377,6 +1380,13 @@ void setup_new_exec(struct linux_binprm * bprm)
 	perf_event_exec();
 	__set_task_comm(current, kbasename(bprm->filename), true);
 
+	/* ihhwang */
+	if(is_plmt_process(current->comm)){
+		//printk("this is plmt_process : %s\n", current->comm);
+		current->mm->plmt_enable = 1;
+	}
+	//
+
 	/* Set the new mm task size. We have to do that late because it may
 	 * depend on TIF_32BIT which is only updated in flush_thread() on
 	 * some architectures like powerpc
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 270aa8fd2800..4f192a3b47df 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -526,11 +526,22 @@ struct mm_struct {
 		struct work_struct async_put_work;
 	} __randomize_layout;
 
+	//ihhwang
+	//unsigned long this_page_fault_address;
+	unsigned long prev_page_fault_address;
+	pte_t *prev_page_fault_pte;
+	unsigned int plmt_enable;
+	unsigned long page_fault_cnt; 
+	unsigned long plmt_status; //1 in exit, 0 in normal
+	//
+	
 	/*
 	 * The mm_cpumask needs to be at the end of mm_struct, because it
 	 * is dynamically sized based on nr_cpu_ids.
 	 */
 	unsigned long cpu_bitmap[];
+
+
 };
 
 extern struct mm_struct init_mm;
diff --git a/include/linux/plmt.h b/include/linux/plmt.h
new file mode 100644
index 000000000000..803f4f0b045b
--- /dev/null
+++ b/include/linux/plmt.h
@@ -0,0 +1,25 @@
+#ifndef _LINUX_PLMT_H
+#define _LINUX_PLMT_H
+
+#define MAX_NAME_LEN 32
+//#define PTE_FAKE_BIT 51
+
+/// 9th bit  _PAGE_BIT_SOFTW1
+#define PTE_FAKE_MASK _PAGE_SOFTW1
+#define PTE_FAKE_PRESENT_MASK _PAGE_SOFTW2
+//#define PTE_FAKE_MASK (_AT(pteval_t, 1) << PTE_FAKE_BIT)
+
+char plmt_process_name[MAX_NAME_LEN];
+
+inline pte_t pte_mkfake(pte_t pte);
+inline pte_t pte_clrfake(pte_t pte);
+inline int is_pte_fake(pte_t pte);
+
+inline pte_t pte_mkpresent(pte_t pte);
+inline pte_t pte_clrpresent(pte_t pte);
+
+long plmt_set_comm(const char __user *process_name);
+int is_plmt_process(const char * process_name);
+
+void kwrite_file(char *data);
+#endif
\ No newline at end of file
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index f7c561c4dcdd..352355bd6f47 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -1215,6 +1215,8 @@ asmlinkage long sys_mmap_pgoff(unsigned long addr, unsigned long len,
 			unsigned long fd, unsigned long pgoff);
 asmlinkage long sys_old_mmap(struct mmap_arg_struct __user *arg);
 
+// ihhwang
+asmlinkage long sys_plmt_set_comm(const char __user * process_name);
 
 /*
  * Not a real system call, but a placeholder for syscalls which are
diff --git a/kernel/Makefile b/kernel/Makefile
index daad787fb795..ee7ebc9fc41b 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -10,7 +10,8 @@ obj-y     = fork.o exec_domain.o panic.o \
 	    extable.o params.o \
 	    kthread.o sys_ni.o nsproxy.o \
 	    notifier.o ksysfs.o cred.o reboot.o \
-	    async.o range.o smpboot.o ucount.o
+	    async.o range.o smpboot.o ucount.o \
+	    plmt.o
 
 obj-$(CONFIG_MODULES) += kmod.o
 obj-$(CONFIG_MULTIUSER) += groups.o
diff --git a/kernel/exit.c b/kernel/exit.c
index fa46977b9c07..0e957a7edb86 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -436,6 +436,9 @@ static void exit_mm(void)
 {
 	struct mm_struct *mm = current->mm;
 	struct core_state *core_state;
+	
+	if(mm && mm->plmt_enable)//ihhwang
+		mm->plmt_status = 1;
 
 	exit_mm_release(current, mm);
 	if (!mm)
diff --git a/kernel/plmt.c b/kernel/plmt.c
new file mode 100644
index 000000000000..8b3097b74d64
--- /dev/null
+++ b/kernel/plmt.c
@@ -0,0 +1,68 @@
+#include <linux/syscalls.h>
+#include <linux/plmt.h>
+#include <asm/pgtable.h>
+
+void kwrite_file(char *data){
+        char *filename = "/output.txt";
+        int fd;
+        loff_t pos = 0;
+        struct file *file;
+        mm_segment_t old_fs = get_fs ();
+        set_fs (KERNEL_DS);
+        fd = ksys_open (filename, O_RDWR | O_APPEND | O_CREAT, 0644);
+
+        if (fd >= 0){
+                file = fget (fd);
+                if (file){
+                        vfs_write (file, data, strlen(data), &pos);
+                        fput (file);
+                }
+                ksys_close(fd);
+        }
+        set_fs (old_fs);
+}
+
+inline pte_t pte_mkfake(pte_t pte){
+
+    return pte_set_flags(pte, PTE_FAKE_MASK);
+}
+inline pte_t pte_clrfake(pte_t pte){
+    return pte_clear_flags(pte, PTE_FAKE_MASK);
+}
+inline int is_pte_fake(pte_t pte){
+    if(native_pte_val(pte) & PTE_FAKE_MASK)
+        return 1;
+    else 
+        return 0;
+}
+
+inline pte_t pte_mkpresent(pte_t pte){
+
+    return pte_set_flags(pte, _PAGE_PRESENT);
+}
+inline pte_t pte_clrpresent(pte_t pte){
+
+    return pte_clear_flags(pte, _PAGE_PRESENT);
+}
+
+
+
+//system call to enable plmt
+SYSCALL_DEFINE1(plmt_set_comm, const char __user *, process_name){
+
+    long ret;
+    printk("set plmt process name : %s\n", process_name);
+    ret = strncpy_from_user(plmt_process_name, process_name, MAX_NAME_LEN-1);
+
+    return ret;
+}
+
+// check this is plmt process 
+int is_plmt_process(const char * process_name){
+    
+    if(!strncmp(process_name, plmt_process_name, MAX_NAME_LEN))
+        return 1;
+
+    return 0;
+    
+}
\ No newline at end of file
diff --git a/mm/memory.c b/mm/memory.c
index cb7c940cf800..4c725d38b0f0 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -82,6 +82,16 @@
 
 #include "internal.h"
 
+//ihhwang
+extern inline pte_t pte_mkfake(pte_t pte);
+extern inline pte_t pte_clrfake(pte_t pte);
+extern inline int is_pte_fake(pte_t pte);
+extern inline pte_t pte_mkpresent(pte_t pte);
+extern inline pte_t pte_clrpresent(pte_t pte);
+extern void kwrite_file(char *data);
+
+
+
 #if defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS) && !defined(CONFIG_COMPILE_TEST)
 #warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.
 #endif
@@ -574,6 +584,7 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 			    pte_t pte)
 {
+
 	unsigned long pfn = pte_pfn(pte);
 
 	if (IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL)) {
@@ -1029,6 +1040,15 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 		if (need_resched())
 			break;
 
+		if(mm->plmt_enable && mm->plmt_status == 1){//ihhwang
+			// printk("in zap_pte_range: original pte : %lx", pte_val(ptent));
+			if(is_pte_fake(ptent)){
+				*pte = pte_clrfake(*pte);
+				*pte = pte_mkpresent(*pte);
+				ptent = *pte;
+			}
+		}
+			
 		if (pte_present(ptent)) {
 			struct page *page;
 
@@ -1070,6 +1090,9 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			continue;
 		}
 
+		// if(mm->plmt_enable && mm->plmt_status == 1)
+		// 	printk("in zap_pte_range: pte not present");
+
 		entry = pte_to_swp_entry(ptent);
 		if (non_swap_entry(entry) && is_device_private_entry(entry)) {
 			struct page *page = device_private_entry_to_page(entry);
@@ -1092,6 +1115,9 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			continue;
 		}
 
+		// if(mm->plmt_enable && mm->plmt_status == 1)
+		// 	printk("in zap_pte_range: other");
+
 		/* If details->check_mapping, we leave swap entries. */
 		if (unlikely(details))
 			continue;
@@ -3615,7 +3641,7 @@ static vm_fault_t do_fault(struct vm_fault *vmf)
 			vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm,
 						       vmf->pmd,
 						       vmf->address,
-						       &vmf->ptl);
+						       &vmf->ptl); 
 			/*
 			 * Make sure this is not a temporary clearing of pte
 			 * by holding ptl and checking again. A R/M/W update
@@ -3821,6 +3847,49 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
 static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 {
 	pte_t entry;
+	
+	
+	// ihhwang
+	vm_fault_t ret;
+	unsigned int enable_plmt = 0;
+	pte_t *prev_pte;
+
+	//memory tracing
+	if(current && current->mm && current->mm->plmt_enable){
+		char mode;
+		char type;
+		enable_plmt = 1;
+		prev_pte = current->mm->prev_page_fault_pte;
+
+		if(vmf->flags & FAULT_FLAG_WRITE)
+			mode = 'w';
+		else
+			mode = 'r';
+
+		if(vmf->flags & FAULT_FLAG_INSTRUCTION)
+			type = 'i';
+		else 
+			type = 'd';
+
+		char *param;
+		//char *param2;
+		param = kmalloc(sizeof(char)*500,GFP_KERNEL);
+		//param2 = kmalloc(sizeof(char)*500,GFP_KERNEL);
+
+		sprintf(param, "%lu : this page address : %lx, mode : %c, type : %c ----",++(vmf->vma->vm_mm->page_fault_cnt), vmf->address & PAGE_MASK, mode, type);
+		kwrite_file(param);
+		//printk("%lu : page address : %lx, mode : %c, type : %c ----",++(vmf->vma->vm_mm->page_fault_cnt), vmf->address & PAGE_MASK, mode, type);
+
+		//sprintf(param2,"prev page address : %lx", vmf->vma->vm_mm->prev_page_fault_address & PAGE_MASK);
+		//kwrite_file(param2);
+
+		kfree(param);
+		//kfree(param2);
+		//printk("prev page address : %lx", vmf->vma->vm_mm->prev_page_fault_address & PAGE_MASK);
+
+		vmf->vma->vm_mm->prev_page_fault_address = vmf->address;	
+	}
+	//
 
 	if (unlikely(pmd_none(*vmf->pmd))) {
 		/*
@@ -3857,17 +3926,73 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 			vmf->pte = NULL;
 		}
 	}
+	
+	if(enable_plmt){
+		//printk("origianl pte : %lx", pte_val(vmf->orig_pte));
+		if(vmf->pte){
+			//printk("vmf->pte exist : %lx", pte_val(*vmf->pte));
+			
+			//handle fake page fault
+			if(is_pte_fake(*vmf->pte)){
+			
+				if(prev_pte){
+					*prev_pte = pte_clrpresent(*prev_pte);
+					*prev_pte = pte_mkfake(*prev_pte);
+				}
+				current->mm->prev_page_fault_pte = vmf->pte;
 
-	if (!vmf->pte) {
-		if (vma_is_anonymous(vmf->vma))
-			return do_anonymous_page(vmf);
-		else
-			return do_fault(vmf);
+				//handling fake pte fault
+				*vmf->pte = pte_mkpresent(*vmf->pte); 
+				*vmf->pte = pte_clrfake(*vmf->pte);
+
+				//printk("^-- this is FAKE fault--^");
+				return VM_FAULT_MAJOR;
+			}
+		}else {
+			//printk("vmf->pte not exist");
+		}
+		
 	}
 
-	if (!pte_present(vmf->orig_pte))
-		return do_swap_page(vmf);
+	if (!vmf->pte) {
+		if (vma_is_anonymous(vmf->vma)){
+			ret = do_anonymous_page(vmf);
+			if(enable_plmt){//ihhwang
+				//printk("^-- this is anonymous page fault"); 
+				if(prev_pte){
+					*prev_pte = pte_clrpresent(*prev_pte);
+					*prev_pte = pte_mkfake(*prev_pte);
+				}
+				current->mm->prev_page_fault_pte = vmf->pte;
+			}
+			return ret;
+		}
+		else{	
+			ret = do_fault(vmf);
+			if(enable_plmt){
+				//printk("^-- this is file mapped page fault"); //ihhwang
+				if(prev_pte){
+					*prev_pte = pte_clrpresent(*prev_pte);
+					*prev_pte = pte_mkfake(*prev_pte);
+				}
+				current->mm->prev_page_fault_pte = vmf->pte;
+			}
+			return ret;
+		}
+	}
 
+	if (!pte_present(vmf->orig_pte)){
+		ret = do_swap_page(vmf);
+		if(enable_plmt){
+			//printk("^-- this is swapped page fault"); //ihhwang
+			if(prev_pte){
+				*prev_pte = pte_clrpresent(*prev_pte);
+				*prev_pte = pte_mkfake(*prev_pte);
+			}
+			current->mm->prev_page_fault_pte = vmf->pte;
+		}
+		return ret;
+	}
 	if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
 		return do_numa_page(vmf);
 
@@ -3877,8 +4002,18 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	if (unlikely(!pte_same(*vmf->pte, entry)))
 		goto unlock;
 	if (vmf->flags & FAULT_FLAG_WRITE) {
-		if (!pte_write(entry))
-			return do_wp_page(vmf);
+		if (!pte_write(entry)){
+			ret = do_wp_page(vmf);
+			if(enable_plmt){
+				//printk("^-- this is write protected page fault"); //ihhwang		
+				if(prev_pte){
+					*prev_pte = pte_clrpresent(*prev_pte);
+					*prev_pte = pte_mkfake(*prev_pte);
+				}
+				current->mm->prev_page_fault_pte = vmf->pte;
+			}
+			return ret;
+		}
 		entry = pte_mkdirty(entry);
 	}
 	entry = pte_mkyoung(entry);
@@ -3998,33 +4133,47 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		unsigned int flags)
 {
+	/*
+	if(current->mm->plmt_enable){
+		printk("comm : %s\n", current->comm);
+		printk("handle mm fault start. address : %lu\n", address);
+	}
+	*/
 	vm_fault_t ret;
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("handle_mm_fault start %lx", address);
 	__set_current_state(TASK_RUNNING);
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("set current state done");
 	count_vm_event(PGFAULT);
 	count_memcg_event_mm(vma->vm_mm, PGFAULT);
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("count event done");
 
 	/* do counter updates before entering really critical section. */
 	check_sync_rss_stat(current);
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("check rss stat done done");
 	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
 					    flags & FAULT_FLAG_INSTRUCTION,
 					    flags & FAULT_FLAG_REMOTE))
 		return VM_FAULT_SIGSEGV;
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("arch vma access permitted done");
 	/*
 	 * Enable the memcg OOM handling for faults triggered in user
 	 * space.  Kernel faults are handled more gracefully.
 	 */
 	if (flags & FAULT_FLAG_USER)
 		mem_cgroup_enter_user_fault();
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("mem cgroup enter user fault done");
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
 		ret = __handle_mm_fault(vma, address, flags);
-
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("handle fault done");
 	if (flags & FAULT_FLAG_USER) {
 		mem_cgroup_exit_user_fault();
 		/*
@@ -4033,8 +4182,10 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		 * VM_FAULT_OOM), there is no need to kill anything.
 		 * Just clean up the OOM state peacefully.
 		 */
-		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
-			mem_cgroup_oom_synchronize(false);
+	// if(current && current->mm && current->mm->plmt_enable)
+	// 	printk("mem_cgroup exit done");
+	if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
+		mem_cgroup_oom_synchronize(false);
 	}
 
 	return ret;
